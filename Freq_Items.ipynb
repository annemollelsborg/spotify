{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "- Load cleaned data and user data with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import ast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_songs = \"Data/df_combined.csv\"\n",
    "df_songs = pd.read_csv(file_path_songs)\n",
    "file_path_clusters = \"Data/df_users.csv\"\n",
    "df_users = pd.read_csv(file_path_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute market basket data frames\n",
    "- A market basket data frame is computed for each cluster\n",
    "- Only users with more than 50 total listenings are included\n",
    "- Each basket includes at most the users 100 most played songs\n",
    "- Songs which does not occur in more than 10 different baskets are excluded\n",
    "\n",
    "We generate 11 separate DataFrames stored as `df_basket_1`, `df_basket_2`, ..., `df_basket_11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total play count for each user\n",
    "user_total_playcount = df_users.groupby('user_id')['playcount'].sum().reset_index()\n",
    "user_total_playcount = user_total_playcount[user_total_playcount['playcount'] >= 50]\n",
    "\n",
    "# Filter the original user data to include only users with at least 50 total plays\n",
    "df_users_filtered = df_users[df_users['user_id'].isin(user_total_playcount['user_id'])]\n",
    "\n",
    "# Process each cluster to generate individual DataFrames\n",
    "cluster_dataframes = {}  # Dictionary to hold the dataframes for each cluster\n",
    "\n",
    "for cluster_id in sorted(df_users_filtered['most_played_cluster'].unique()[:11]):  # Ensure max 11 clusters\n",
    "    # Filter the data for the current cluster\n",
    "    cluster_data = df_users_filtered[df_users_filtered['most_played_cluster'] == cluster_id]\n",
    "\n",
    "    # Sort songs for each user by play count and select the 100 most played songs\n",
    "    def get_top_songs(user_data):\n",
    "        return user_data.nlargest(100, 'playcount')  # Adjust to desired number of top songs\n",
    "\n",
    "    # Apply the top song selection for each user\n",
    "    df_top_songs_cluster = cluster_data.groupby('user_id').apply(get_top_songs).reset_index(drop=True)\n",
    "\n",
    "    # Create a basket for each user with their top songs\n",
    "    df_basket_cluster = df_top_songs_cluster.groupby('user_id')['track_id'].apply(list).reset_index()\n",
    "    df_basket_cluster.rename(columns={'track_id': 'basket'}, inplace=True)\n",
    "\n",
    "    # Assign the DataFrame to a variable dynamically\n",
    "    cluster_dataframes[f\"df_basket_{cluster_id}\"] = df_basket_cluster\n",
    "\n",
    "    # Optionally, save to a named variable\n",
    "    globals()[f\"df_basket_{cluster_id}\"] = df_basket_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter songs occurring in more than 10 baskets\n",
    "def filter_songs(df_basket, min_basket_count=10):\n",
    "    \"\"\"\n",
    "    Filters out songs from the baskets that occur in fewer than `min_basket_count` baskets.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_basket (pd.DataFrame): DataFrame with columns `user_id` and `basket`.\n",
    "    - min_basket_count (int): Minimum number of baskets a song must appear in to be retained.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame with updated baskets.\n",
    "    \"\"\"\n",
    "    # Flatten all baskets into a single list to count song occurrences\n",
    "    all_songs = [song for basket in df_basket['basket'] for song in basket]\n",
    "    song_counts = Counter(all_songs)\n",
    "\n",
    "    # Identify songs that occur in more than `min_basket_count` baskets\n",
    "    frequent_songs = {song for song, count in song_counts.items() if count > min_basket_count}\n",
    "\n",
    "    # Filter each user's basket to retain only frequent songs\n",
    "    df_basket['basket'] = df_basket['basket'].apply(lambda basket: [song for song in basket if song in frequent_songs])\n",
    "\n",
    "    # Remove rows with empty baskets\n",
    "    df_basket = df_basket[df_basket['basket'].map(len) > 0].reset_index(drop=True)\n",
    "\n",
    "    return df_basket\n",
    "\n",
    "# Apply the function to each df_basket_# DataFrame\n",
    "for cluster_id in cluster_dataframes.keys():\n",
    "    cluster_dataframes[cluster_id] = filter_songs(cluster_dataframes[cluster_id])\n",
    "\n",
    "    # Optionally, save back to dynamically created variables\n",
    "    globals()[cluster_id] = cluster_dataframes[cluster_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute association rules and support\n",
    "- Support and association rules are computed for each basket data frame\n",
    "- The minimum support used is 2%\n",
    "- Association rules are computed using the Apriori algorithm to generate frequent single items, dublets, and triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate support for itemsets\n",
    "def calculate_support(data, itemsets):\n",
    "    itemset_counts = defaultdict(int)\n",
    "    for basket in data['basket']:\n",
    "        for itemset in itemsets:\n",
    "            if set(itemset).issubset(set(basket)):\n",
    "                itemset_counts[itemset] += 1\n",
    "    total_baskets = len(data)\n",
    "    support = {itemset: count / total_baskets for itemset, count in itemset_counts.items()}\n",
    "    return support\n",
    "\n",
    "# Apriori Algorithm for Size 2 and 3\n",
    "def apriori_pairs_and_triplets(data, min_support):\n",
    "    frequent_itemsets = {}\n",
    "\n",
    "    # Start with single items\n",
    "    items = set(item for basket in data['basket'] for item in basket)\n",
    "    single_itemsets = [(item,) for item in items]\n",
    "    \n",
    "    # Calculate support for single items\n",
    "    support = calculate_support(data, single_itemsets)\n",
    "    frequent_singles = [itemset for itemset, sup in support.items() if sup >= min_support]\n",
    "    frequent_itemsets.update({itemset: support[itemset] for itemset in frequent_singles})\n",
    "    \n",
    "    print(f\"Frequent Single Items: {frequent_singles}\")\n",
    "    \n",
    "    # Generate pairs (size 2)\n",
    "    pairs = list(combinations(set(item for itemset in frequent_singles for item in itemset), 2))\n",
    "    pair_support = calculate_support(data, pairs)\n",
    "    frequent_pairs = [itemset for itemset, sup in pair_support.items() if sup >= min_support]\n",
    "    frequent_itemsets.update({itemset: pair_support[itemset] for itemset in frequent_pairs})\n",
    "    \n",
    "    print(f\"Frequent Pairs: {frequent_pairs}\")\n",
    "    \n",
    "    # Generate triplets (size 3)\n",
    "    triplets = list(combinations(set(item for itemset in frequent_pairs for item in itemset), 3))\n",
    "    triplet_support = calculate_support(data, triplets)\n",
    "    frequent_triplets = [itemset for itemset, sup in triplet_support.items() if sup >= min_support]\n",
    "    frequent_itemsets.update({itemset: triplet_support[itemset] for itemset in frequent_triplets})\n",
    "    \n",
    "    print(f\"Frequent Triplets: {frequent_triplets}\")\n",
    "    \n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate results (freq. items and support)\n",
    "The frequent items and item sets with corresponding support are stored in a CSV file for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing df_basket_1...\n",
      "Frequent Single Items: [('TRRXGAK128EF349F1A',), ('TRRKGRC128F932D8F0',), ('TRWZFIC128F933BCA3',), ('TRNEWWX128F9336A1F',), ('TRUFTBY128F93450B8',), ('TRCPXID128F92D5D3C',), ('TRLUKKL128F4284EEA',), ('TRIJLQJ128E078F6F1',), ('TROYOWO12903CACF51',), ('TRUNCXA12903CDAA07',), ('TRBEFXF128F4263CE9',), ('TRONYHY128F92C9D11',), ('TRAZIQK12903CCFB3A',), ('TRRGWHY128F93043DB',), ('TRUDNRB128F42598CA',), ('TRBNYBX128F422EC61',), ('TRUWANM128F1485EE2',), ('TRSUNIO128F92DD214',), ('TRFKZHE128F149F632',), ('TRVZILF128F42748EF',), ('TRRGKHJ128F92F64DA',), ('TRBHVEV128F425C018',), ('TRMEBVU128F92F64DB',), ('TRCQFJI128F4284EEE',), ('TRFTUIW128E0784B9F',), ('TRQJLCO128F42BCC0A',), ('TRYFDNR128F4260C5E',), ('TRPDNZQ128F92C1E98',), ('TRXHXZJ128F92DD518',), ('TREXRIW128EF3434B7',), ('TRETTHQ128F428294E',), ('TRERZDK128F42B3222',), ('TRIXKCB128F424EA32',), ('TRZNAHL128F9327D5A',), ('TRIYBSB128F14B0259',), ('TRWCIAX128F42925BD',), ('TRFNGJS128F92F9EEE',), ('TRFWGOJ128E0780C8B',), ('TRPYYZN128F9304FBA',), ('TRHJBZP128F145F6A6',), ('TRSUSWW128F93463BF',), ('TRMVCVS128F1468256',), ('TRPFYYL128F92F7144',), ('TRAOIAH128F92F707B',), ('TRXYRWA128F92CDF2E',), ('TRCGAVX12903CFBC61',), ('TRUNRNB128F42A76B6',), ('TRUYKXX128F426C2F1',), ('TRPGPDK12903CCC651',), ('TRXBXZT128F42A8E06',), ('TRRIRMX128F92E6272',), ('TRWVOJJ12903CCC654',), ('TRAMFJR128F92C1DB1',), ('TRWTPYD12903D03AE9',), ('TRVSJOM12903CD2DC1',), ('TRCTYEY128F428E848',), ('TRDHBPQ128F425EA1E',), ('TROVEVL128F932A447',), ('TRHMVOQ128F1472E6D',), ('TROIBHC128F14958FC',), ('TRZBZRH128F92FFBFC',), ('TRRMVBR128F92F64BA',), ('TRHUNZT128C719654E',), ('TRISTGF12903D0DC6F',), ('TROQTGM128F426733C',), ('TRXWAZC128F9314B3E',), ('TROIRNZ12903CE5D91',), ('TRLRGVX128E078EC1B',), ('TRSGHGP12903CE520A',), ('TRAYVEL128F149F631',), ('TRAALAH128E078234A',), ('TRIXXYP128C7196826',), ('TROQETL128F4267337',), ('TRFRGTX12903CF7AB1',), ('TRZGLNO128F92D65BE',), ('TRRMGWC128F426733E',), ('TREBWYU12903CCDAA8',), ('TRRGZTI128F4255806',), ('TRUCRUF128F14B0255',), ('TRZALGW128F4296174',), ('TRWNPDZ128F934B946',), ('TROSQNP128F92C5073',), ('TRXLBFN128EF343BFE',), ('TRZNGII128C7196554',), ('TRRJQES128EF35675B',), ('TRXXCIZ128F42574DD',), ('TRABFDT12903CADD73',), ('TRLOHWT128F42598CF',), ('TRGCHLH12903CB7352',), ('TRIPUHC12903D03AFC',), ('TRLVQME128F931BAF3',), ('TRWAQOC12903CB84CA',), ('TRVSFYH128F42639B7',), ('TRZJEUW128F93456CB',), ('TRZXJMS128F4259AFC',), ('TRJAAFV128F426C35F',), ('TRNXZHW128F934B958',), ('TRGNDNE128F92E50F5',), ('TREHENJ128E07953C7',), ('TRAAKDG128F42A0ECB',), ('TRYYTUP128F4298472',), ('TRUEQWJ128F421CD83',), ('TRRVJCK12903CD2DCB',), ('TRZLEQU128F4259AFD',), ('TRMFXMQ128F4259B01',), ('TRKXOWA128EF3434B2',), ('TRBARSX12903D03AE5',), ('TREFMGX128F4259AFB',), ('TRDBYEH12903D03AF3',), ('TRGGDOF12903D03AEE',), ('TRBKFKL128E078ED76',), ('TRJJQNH128F92CEB62',), ('TRWIKBF128F92DD506',), ('TRRVSBS128F425A7B0',), ('TROMKCG128F9320C09',), ('TRASUJG128F92D6316',), ('TRLUROI128F92FFBFD',), ('TRYPVYH128F42A4C53',), ('TRSHTTA128F1469578',), ('TRXHAZS128F42598D1',), ('TRBJARK128F1465649',), ('TRXESGJ12903CC04D2',)]\n",
      "Frequent Pairs: [('TRUFTBY128F93450B8', 'TRCPXID128F92D5D3C'), ('TRMEBVU128F92F64DB', 'TRRGKHJ128F92F64DA'), ('TRETTHQ128F428294E', 'TRXHXZJ128F92DD518'), ('TRHMVOQ128F1472E6D', 'TROVEVL128F932A447'), ('TRHMVOQ128F1472E6D', 'TRFKZHE128F149F632'), ('TROVEVL128F932A447', 'TRFKZHE128F149F632'), ('TRRMVBR128F92F64BA', 'TRHUNZT128C719654E'), ('TRRMVBR128F92F64BA', 'TRMEBVU128F92F64DB'), ('TRRMVBR128F92F64BA', 'TRUYKXX128F426C2F1'), ('TRRMVBR128F92F64BA', 'TRWCIAX128F42925BD'), ('TRRMVBR128F92F64BA', 'TRRGKHJ128F92F64DA'), ('TRRMVBR128F92F64BA', 'TRISTGF12903D0DC6F'), ('TRHUNZT128C719654E', 'TRMEBVU128F92F64DB'), ('TRHUNZT128C719654E', 'TRUYKXX128F426C2F1'), ('TRHUNZT128C719654E', 'TRWCIAX128F42925BD'), ('TRHUNZT128C719654E', 'TRRGKHJ128F92F64DA'), ('TRHUNZT128C719654E', 'TRISTGF12903D0DC6F'), ('TRMEBVU128F92F64DB', 'TRUYKXX128F426C2F1'), ('TRMEBVU128F92F64DB', 'TRWCIAX128F42925BD'), ('TRMEBVU128F92F64DB', 'TRISTGF12903D0DC6F'), ('TRUYKXX128F426C2F1', 'TRWCIAX128F42925BD'), ('TRUYKXX128F426C2F1', 'TRRGKHJ128F92F64DA'), ('TRUYKXX128F426C2F1', 'TRISTGF12903D0DC6F'), ('TRWCIAX128F42925BD', 'TRRGKHJ128F92F64DA'), ('TRWCIAX128F42925BD', 'TRISTGF12903D0DC6F'), ('TRRGKHJ128F92F64DA', 'TRISTGF12903D0DC6F'), ('TRHMVOQ128F1472E6D', 'TRAYVEL128F149F631'), ('TRAYVEL128F149F631', 'TRFKZHE128F149F632'), ('TROVEVL128F932A447', 'TRAYVEL128F149F631'), ('TRRMVBR128F92F64BA', 'TRZNGII128C7196554'), ('TRZNGII128C7196554', 'TRHUNZT128C719654E'), ('TRZNGII128C7196554', 'TRMEBVU128F92F64DB'), ('TRZNGII128C7196554', 'TRWCIAX128F42925BD'), ('TRZNGII128C7196554', 'TRRGKHJ128F92F64DA'), ('TRZNGII128C7196554', 'TRISTGF12903D0DC6F'), ('TRRJQES128EF35675B', 'TRMEBVU128F92F64DB'), ('TRRMVBR128F92F64BA', 'TRUEQWJ128F421CD83'), ('TRHUNZT128C719654E', 'TRUEQWJ128F421CD83'), ('TRMEBVU128F92F64DB', 'TRUEQWJ128F421CD83'), ('TRWCIAX128F42925BD', 'TRUEQWJ128F421CD83'), ('TRRGKHJ128F92F64DA', 'TRUEQWJ128F421CD83'), ('TRUEQWJ128F421CD83', 'TRISTGF12903D0DC6F'), ('TRZLEQU128F4259AFD', 'TRUDNRB128F42598CA'), ('TRIPUHC12903D03AFC', 'TRWTPYD12903D03AE9'), ('TRIPUHC12903D03AFC', 'TRBARSX12903D03AE5'), ('TRWTPYD12903D03AE9', 'TRBARSX12903D03AE5'), ('TRVSFYH128F42639B7', 'TREXRIW128EF3434B7'), ('TRDBYEH12903D03AF3', 'TRIPUHC12903D03AFC'), ('TRDBYEH12903D03AF3', 'TRGGDOF12903D03AEE'), ('TRDBYEH12903D03AF3', 'TRWTPYD12903D03AE9'), ('TRDBYEH12903D03AF3', 'TRBARSX12903D03AE5'), ('TRIPUHC12903D03AFC', 'TRGGDOF12903D03AEE'), ('TRGGDOF12903D03AEE', 'TRWTPYD12903D03AE9'), ('TRLUROI128F92FFBFD', 'TRUDNRB128F42598CA'), ('TRLUROI128F92FFBFD', 'TRLOHWT128F42598CF'), ('TRUDNRB128F42598CA', 'TRLOHWT128F42598CF'), ('TRZLEQU128F4259AFD', 'TRXHAZS128F42598D1'), ('TRZLEQU128F4259AFD', 'TRLOHWT128F42598CF'), ('TRXHAZS128F42598D1', 'TRUDNRB128F42598CA'), ('TRXHAZS128F42598D1', 'TRLOHWT128F42598CF')]\n",
      "Frequent Triplets: [('TROVEVL128F932A447', 'TRHMVOQ128F1472E6D', 'TRFKZHE128F149F632'), ('TRRMVBR128F92F64BA', 'TRWCIAX128F42925BD', 'TRRGKHJ128F92F64DA'), ('TRRMVBR128F92F64BA', 'TRWCIAX128F42925BD', 'TRHUNZT128C719654E'), ('TRRMVBR128F92F64BA', 'TRWCIAX128F42925BD', 'TRMEBVU128F92F64DB'), ('TRRMVBR128F92F64BA', 'TRWCIAX128F42925BD', 'TRUYKXX128F426C2F1'), ('TRRMVBR128F92F64BA', 'TRWCIAX128F42925BD', 'TRISTGF12903D0DC6F'), ('TRRMVBR128F92F64BA', 'TRRGKHJ128F92F64DA', 'TRHUNZT128C719654E'), ('TRRMVBR128F92F64BA', 'TRRGKHJ128F92F64DA', 'TRMEBVU128F92F64DB'), ('TRRMVBR128F92F64BA', 'TRRGKHJ128F92F64DA', 'TRUYKXX128F426C2F1'), ('TRRMVBR128F92F64BA', 'TRRGKHJ128F92F64DA', 'TRISTGF12903D0DC6F'), ('TRRMVBR128F92F64BA', 'TRHUNZT128C719654E', 'TRMEBVU128F92F64DB'), ('TRRMVBR128F92F64BA', 'TRHUNZT128C719654E', 'TRUYKXX128F426C2F1'), ('TRRMVBR128F92F64BA', 'TRHUNZT128C719654E', 'TRISTGF12903D0DC6F'), ('TRRMVBR128F92F64BA', 'TRMEBVU128F92F64DB', 'TRUYKXX128F426C2F1'), ('TRRMVBR128F92F64BA', 'TRMEBVU128F92F64DB', 'TRISTGF12903D0DC6F'), ('TRRMVBR128F92F64BA', 'TRUYKXX128F426C2F1', 'TRISTGF12903D0DC6F'), ('TRWCIAX128F42925BD', 'TRRGKHJ128F92F64DA', 'TRHUNZT128C719654E'), ('TRWCIAX128F42925BD', 'TRRGKHJ128F92F64DA', 'TRMEBVU128F92F64DB'), ('TRWCIAX128F42925BD', 'TRRGKHJ128F92F64DA', 'TRUYKXX128F426C2F1'), ('TRWCIAX128F42925BD', 'TRRGKHJ128F92F64DA', 'TRISTGF12903D0DC6F'), ('TRWCIAX128F42925BD', 'TRHUNZT128C719654E', 'TRMEBVU128F92F64DB'), ('TRWCIAX128F42925BD', 'TRHUNZT128C719654E', 'TRUYKXX128F426C2F1'), ('TRWCIAX128F42925BD', 'TRHUNZT128C719654E', 'TRISTGF12903D0DC6F'), ('TRWCIAX128F42925BD', 'TRMEBVU128F92F64DB', 'TRUYKXX128F426C2F1'), ('TRWCIAX128F42925BD', 'TRMEBVU128F92F64DB', 'TRISTGF12903D0DC6F'), ('TRWCIAX128F42925BD', 'TRUYKXX128F426C2F1', 'TRISTGF12903D0DC6F'), ('TRRGKHJ128F92F64DA', 'TRHUNZT128C719654E', 'TRMEBVU128F92F64DB'), ('TRRGKHJ128F92F64DA', 'TRHUNZT128C719654E', 'TRUYKXX128F426C2F1'), ('TRRGKHJ128F92F64DA', 'TRHUNZT128C719654E', 'TRISTGF12903D0DC6F'), ('TRRGKHJ128F92F64DA', 'TRMEBVU128F92F64DB', 'TRUYKXX128F426C2F1'), ('TRRGKHJ128F92F64DA', 'TRMEBVU128F92F64DB', 'TRISTGF12903D0DC6F'), ('TRRGKHJ128F92F64DA', 'TRUYKXX128F426C2F1', 'TRISTGF12903D0DC6F'), ('TRHUNZT128C719654E', 'TRMEBVU128F92F64DB', 'TRUYKXX128F426C2F1'), ('TRHUNZT128C719654E', 'TRMEBVU128F92F64DB', 'TRISTGF12903D0DC6F'), ('TRHUNZT128C719654E', 'TRUYKXX128F426C2F1', 'TRISTGF12903D0DC6F'), ('TRMEBVU128F92F64DB', 'TRUYKXX128F426C2F1', 'TRISTGF12903D0DC6F'), ('TRHMVOQ128F1472E6D', 'TRAYVEL128F149F631', 'TRFKZHE128F149F632'), ('TROVEVL128F932A447', 'TRHMVOQ128F1472E6D', 'TRAYVEL128F149F631'), ('TROVEVL128F932A447', 'TRAYVEL128F149F631', 'TRFKZHE128F149F632'), ('TRRMVBR128F92F64BA', 'TRZNGII128C7196554', 'TRWCIAX128F42925BD'), ('TRRMVBR128F92F64BA', 'TRZNGII128C7196554', 'TRRGKHJ128F92F64DA'), ('TRRMVBR128F92F64BA', 'TRZNGII128C7196554', 'TRHUNZT128C719654E'), ('TRRMVBR128F92F64BA', 'TRZNGII128C7196554', 'TRMEBVU128F92F64DB'), ('TRRMVBR128F92F64BA', 'TRZNGII128C7196554', 'TRISTGF12903D0DC6F'), ('TRZNGII128C7196554', 'TRHUNZT128C719654E', 'TRMEBVU128F92F64DB'), ('TRZNGII128C7196554', 'TRHUNZT128C719654E', 'TRISTGF12903D0DC6F'), ('TRZNGII128C7196554', 'TRMEBVU128F92F64DB', 'TRISTGF12903D0DC6F'), ('TRRMVBR128F92F64BA', 'TRWCIAX128F42925BD', 'TRUEQWJ128F421CD83'), ('TRRMVBR128F92F64BA', 'TRRGKHJ128F92F64DA', 'TRUEQWJ128F421CD83'), ('TRRMVBR128F92F64BA', 'TRHUNZT128C719654E', 'TRUEQWJ128F421CD83'), ('TRRMVBR128F92F64BA', 'TRUEQWJ128F421CD83', 'TRISTGF12903D0DC6F'), ('TRWCIAX128F42925BD', 'TRHUNZT128C719654E', 'TRUEQWJ128F421CD83'), ('TRRGKHJ128F92F64DA', 'TRHUNZT128C719654E', 'TRUEQWJ128F421CD83'), ('TRHUNZT128C719654E', 'TRMEBVU128F92F64DB', 'TRUEQWJ128F421CD83'), ('TRHUNZT128C719654E', 'TRUEQWJ128F421CD83', 'TRISTGF12903D0DC6F'), ('TRMEBVU128F92F64DB', 'TRUEQWJ128F421CD83', 'TRISTGF12903D0DC6F'), ('TRIPUHC12903D03AFC', 'TRBARSX12903D03AE5', 'TRWTPYD12903D03AE9'), ('TRGGDOF12903D03AEE', 'TRDBYEH12903D03AF3', 'TRIPUHC12903D03AFC'), ('TRGGDOF12903D03AEE', 'TRDBYEH12903D03AF3', 'TRWTPYD12903D03AE9'), ('TRGGDOF12903D03AEE', 'TRIPUHC12903D03AFC', 'TRWTPYD12903D03AE9'), ('TRDBYEH12903D03AF3', 'TRIPUHC12903D03AFC', 'TRBARSX12903D03AE5'), ('TRDBYEH12903D03AF3', 'TRIPUHC12903D03AFC', 'TRWTPYD12903D03AE9'), ('TRLUROI128F92FFBFD', 'TRUDNRB128F42598CA', 'TRLOHWT128F42598CF'), ('TRXHAZS128F42598D1', 'TRZLEQU128F4259AFD', 'TRUDNRB128F42598CA'), ('TRXHAZS128F42598D1', 'TRZLEQU128F4259AFD', 'TRLOHWT128F42598CF'), ('TRXHAZS128F42598D1', 'TRUDNRB128F42598CA', 'TRLOHWT128F42598CF'), ('TRZLEQU128F4259AFD', 'TRUDNRB128F42598CA', 'TRLOHWT128F42598CF')]\n",
      "Results saved for cluster 1 to Data/Results/cluster_1_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for all baskets\n",
    "df_baskets = {f\"df_basket_{i}\": globals()[f\"df_basket_{i}\"] for i in range(1, 2)}\n",
    "\n",
    "min_support = 0.02\n",
    "\n",
    "# Process each basket and save results\n",
    "for i, (name, data) in enumerate(df_baskets.items(), start=1):\n",
    "    print(f\"Processing {name}...\")\n",
    "    frequent_itemsets = apriori_pairs_and_triplets(data, min_support)\n",
    "    \n",
    "    # Convert frequent itemsets to a DataFrame\n",
    "    results_df = pd.DataFrame(\n",
    "        [{\"itemset\": itemset, \"support\": support} for itemset, support in frequent_itemsets.items()]\n",
    "    )\n",
    "    \n",
    "    # Save each result DataFrame with a unique name\n",
    "    output_file = f\"Data/Results/cluster_{i}_results.csv\"  # Use the index 'i' for unique filenames\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved for cluster {i} to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results: Final songs recommendations\n",
    "- Heatmap showing the support between pairs of songs (duplets)\n",
    "- Network graph showing triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder similarity measure function using song attributes\n",
    "def calculate_similarity(track_id1, track_id2, track_id_to_attributes):\n",
    "    attributes1 = track_id_to_attributes.get(track_id1)\n",
    "    attributes2 = track_id_to_attributes.get(track_id2)\n",
    "    if attributes1 is not None and attributes2 is not None:\n",
    "        # Compute cosine similarity between the attribute vectors\n",
    "        similarity = cosine_similarity([attributes1], [attributes2])[0][0]\n",
    "        return similarity\n",
    "    else:\n",
    "        return 0  # Or handle missing data appropriately\n",
    "\n",
    "# Function to process a single file for heatmaps and graphs\n",
    "def process_csv(file_path, id_to_name, track_id_to_attributes):\n",
    "    # Load duplets and their support values\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Parse the 'itemset' column safely\n",
    "    duplets = {\n",
    "        tuple(item.strip() for item in ast.literal_eval(row[\"itemset\"])): row[\"support\"]\n",
    "        for _, row in df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Filter out pairs including the track ID for 'Revelry' and ensure only pairs are included\n",
    "    # Replace 'track_id_revelry' with the actual track ID for 'Revelry' if you wish to exclude it\n",
    "    track_id_revelry = 'track_id_revelry'  # Placeholder; replace with actual track ID if needed\n",
    "\n",
    "    filtered_duplets = {\n",
    "        pair: support for pair, support in duplets.items()\n",
    "        if track_id_revelry not in pair and len(pair) == 2\n",
    "    }\n",
    "\n",
    "    # Prepare data for support heatmap\n",
    "    track_ids = sorted(list(set([item for pair in filtered_duplets.keys() for item in pair])))\n",
    "    heatmap_data = pd.DataFrame(0, index=track_ids, columns=track_ids)\n",
    "\n",
    "    for (track_id1, track_id2), support in filtered_duplets.items():\n",
    "        heatmap_data.loc[track_id1, track_id2] = support\n",
    "        heatmap_data.loc[track_id2, track_id1] = support  # Symmetric pairs\n",
    "\n",
    "    # Map track IDs to song names for labeling\n",
    "    track_id_to_name = {\n",
    "        track_id: id_to_name.get(track_id, f\"Unknown({track_id})\")\n",
    "        for track_id in track_ids\n",
    "    }\n",
    "    heatmap_data.rename(index=track_id_to_name, columns=track_id_to_name, inplace=True)\n",
    "\n",
    "    # Save the support heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=False,\n",
    "        cmap=\"Blues\",\n",
    "        cbar_kws={'label': 'Support'}\n",
    "    )\n",
    "    plt.title(f\"Song Pair Support Heatmap - {os.path.basename(file_path)}\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    heatmap_path = os.path.join(output_dir_heatmaps, f\"{os.path.basename(file_path).replace('.csv', '')}_heatmap.png\")\n",
    "    plt.savefig(heatmap_path)\n",
    "    plt.close()\n",
    "    print(f\"Support heatmap saved to {heatmap_path}\")\n",
    "\n",
    "    # Prepare data for similarity heatmap using track IDs\n",
    "    similarity_matrix = np.zeros((len(track_ids), len(track_ids)))\n",
    "    for i, track_id1 in enumerate(track_ids):\n",
    "        for j, track_id2 in enumerate(track_ids):\n",
    "            if i <= j:  # Compute only upper triangular and diagonal\n",
    "                similarity = calculate_similarity(track_id1, track_id2, track_id_to_attributes)\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity  # Symmetric matrix\n",
    "\n",
    "    # Create DataFrame for similarity heatmap\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=track_ids, columns=track_ids)\n",
    "    # Map track IDs to song names for labeling\n",
    "    similarity_df.rename(index=track_id_to_name, columns=track_id_to_name, inplace=True)\n",
    "\n",
    "    # Save the similarity heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        similarity_df,\n",
    "        annot=False,\n",
    "        cmap=\"coolwarm_r\",\n",
    "        cbar_kws={'label': 'Similarity'},\n",
    "        xticklabels=True,\n",
    "        yticklabels=True\n",
    "    )\n",
    "    plt.title(f\"Song Similarity Heatmap - {os.path.basename(file_path)}\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    similarity_heatmap_path = os.path.join(output_dir_heatmaps, f\"{os.path.basename(file_path).replace('.csv', '')}_similarity_heatmap.png\")\n",
    "    plt.savefig(similarity_heatmap_path)\n",
    "    plt.close()\n",
    "    print(f\"Similarity heatmap saved to {similarity_heatmap_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster_1_results.csv...\n",
      "Support heatmap saved to Data/Heatmaps/cluster_1_results_heatmap.png\n",
      "Similarity heatmap saved to Data/Heatmaps/cluster_1_results_similarity_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Directories for input and output\n",
    "input_dir = \"Data/Results\"\n",
    "output_dir_heatmaps = \"Data/Heatmaps\"\n",
    "os.makedirs(output_dir_heatmaps, exist_ok=True)\n",
    "\n",
    "# Define the mapping of track IDs to names\n",
    "id_to_name = {\n",
    "    track_id.strip(): name\n",
    "    for track_id, name in df_songs.set_index('track_id')['name'].to_dict().items()\n",
    "}\n",
    "\n",
    "# Prepare a mapping from track IDs to their attributes (with placeholders)\n",
    "# Let's assume the attributes are 'attr1', 'attr2', ..., 'attr6'\n",
    "attribute_columns = ['danceability','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo']\n",
    "\n",
    "# Standardize the attributes\n",
    "scaler = StandardScaler()\n",
    "df_songs[attribute_columns] = scaler.fit_transform(df_songs[attribute_columns])\n",
    "\n",
    "# Create a dictionary mapping track IDs to their attribute vectors\n",
    "track_id_to_attributes = {\n",
    "    track_id: df_songs.loc[df_songs['track_id'] == track_id, attribute_columns].values.flatten()\n",
    "    for track_id in df_songs['track_id']\n",
    "}\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".csv\"):  # Process only CSV files\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        process_csv(file_path, id_to_name, track_id_to_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graph for cluster_1_results.csv...\n",
      "Graph saved for cluster 0 to Data/Graphs/cluster_1_results_group_1.png\n",
      "Graph saved for cluster 1 to Data/Graphs/cluster_1_results_group_2.png\n",
      "Graph saved for cluster 2 to Data/Graphs/cluster_1_results_group_3.png\n",
      "Graph saved for cluster 3 to Data/Graphs/cluster_1_results_group_4.png\n",
      "Graph saved for cluster 4 to Data/Graphs/cluster_1_results_group_5.png\n",
      "Graph saved for cluster 5 to Data/Graphs/cluster_1_results_group_6.png\n",
      "Graph saved for cluster 6 to Data/Graphs/cluster_1_results_group_7.png\n"
     ]
    }
   ],
   "source": [
    "def create_graph_from_results(file_path, id_to_name, output_dir_graphs):\n",
    "    # Load the results file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract triplets and map track IDs to names\n",
    "    triplets = {\n",
    "        tuple(item.strip() for item in row[\"itemset\"].strip(\"()\").replace(\"'\", \"\").split(\", \")): row[\"support\"]\n",
    "        for _, row in df.iterrows()\n",
    "    }\n",
    "    triplets_with_names = [\n",
    "        tuple(id_to_name.get(item, f\"Unknown({item})\") for item in triplet)\n",
    "        for triplet in triplets.keys()\n",
    "    ]\n",
    "\n",
    "    # Create the graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add edges for each triplet\n",
    "    for triplet in triplets_with_names:\n",
    "        for pair in combinations(triplet, 2):\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "\n",
    "    # Identify connected components (clusters)\n",
    "    groups = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "\n",
    "    # Plot each cluster\n",
    "    for i, group in enumerate(groups):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        pos = nx.spring_layout(group, seed=42)  # Use spring layout for neat arrangement\n",
    "        nx.draw_networkx_nodes(group, pos, node_size=1000, node_color='skyblue', alpha=0.9)\n",
    "        nx.draw_networkx_edges(group, pos, width=2, edge_color='gray', alpha=0.6)\n",
    "        nx.draw_networkx_labels(group, pos, font_size=10, font_color='black')\n",
    "        plt.title(f\"Group {i + 1} - {os.path.basename(file_path)}\", fontsize=14)\n",
    "        plt.axis('off')  # Turn off axes\n",
    "\n",
    "        # Save the graph visualization\n",
    "        graph_path = os.path.join(output_dir_graphs, f\"{os.path.basename(file_path).replace('.csv', '')}_group_{i + 1}.png\")\n",
    "        plt.savefig(graph_path)\n",
    "        plt.close()\n",
    "        print(f\"Graph saved for cluster {i} to {graph_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "output_dir_graphs = \"Data/Graphs\"\n",
    "os.makedirs(output_dir_graphs, exist_ok=True)\n",
    "\n",
    "id_to_name = {track_id.strip(): name for track_id, name in df_songs.set_index('track_id')['name'].to_dict().items()}  # Define mapping\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".csv\"):  # Process only CSV files\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        print(f\"Processing graph for {file_name}...\")\n",
    "        create_graph_from_results(file_path, id_to_name, output_dir_graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
