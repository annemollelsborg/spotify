{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             track_id                                   user_id  playcount\n",
      "0  TRIRLYL128F42539D1  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "1  TRFUPBA128F934F7E1  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "2  TRLQPQJ128F42AA94F  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "3  TRTUCUY128F92E1D24  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n",
      "4  TRHDDQG12903CB53EE  b80344d063b5ccb3212f76538f3d9e43d87dca9e          1\n"
     ]
    }
   ],
   "source": [
    "file_path_songs = \"Data/Music Info.csv\"\n",
    "df_songs = pd.read_csv(file_path_songs)\n",
    "file_path_users = \"Data/User Listening History.csv\"\n",
    "df_users = pd.read_csv(file_path_users)\n",
    "\n",
    "print(df_users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['duration_ms', 'danceability', 'energy', 'key',\n",
    "                    'loudness', 'mode', 'speechiness', 'acousticness', \n",
    "                    'instrumentalness', 'liveness', 'valence', \n",
    "                    'tempo', 'time_signature']\n",
    "\n",
    "# Extract the relevant data from the DataFrame\n",
    "music_data = df_songs[selected_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = music_data.corr()\n",
    "\n",
    "# Plot the correlation matrix using seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Music Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df_songs.sample(n = 10000, random_state = 1).isnull(), cmap  = 'viridis', cbar = False, yticklabels = False,\n",
    "            # xticklabels= False\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe this explains why we could use the neural network to predict genre for those observations where the attribute is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: Cluster the user base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we need to cluster the users otherwise we have too many items (songs) when using one hot encoding on the baskets later. We could try cluster on different parameters and see what minimizes the number of items in each basket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Itemsets: Market basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by user_id and aggregate track_ids into a basket\n",
    "df_basket = df_users.groupby('user_id')['track_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter out baskets with fewer than 50 items\n",
    "df_basket = df_basket[df_basket['track_id'].apply(len) > 50]\n",
    "\n",
    "# Rename the column\n",
    "df_basket.rename(columns={'track_id': 'basket'}, inplace=True)\n",
    "\n",
    "print(df_basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_freq = df_users['track_id'].value_counts().reset_index()\n",
    "track_freq.columns = ['track_id', 'frequency']\n",
    "\n",
    "track_freq = track_freq.merge(df_songs, on='track_id')\n",
    "\n",
    "top_15_tracks = track_freq.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_15_tracks['name'], top_15_tracks['frequency'])\n",
    "plt.xlabel('Track Name')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 15 Most Heard Songs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reduce sampling overhead\n",
    "# Instead of using a fraction, use a fixed sample size to ensure stability in runtime\n",
    "sample_size = 10  # Adjust sample size based on available memory and time\n",
    "df_basket_sample = df_basket.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Step 2: Use efficient one-hot encoding\n",
    "# Use SparseDataFrame for memory-efficient encoding\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)  # Enables sparse matrix output\n",
    "one_hot_sparse = mlb.fit_transform(df_basket_sample['basket'])\n",
    "one_hot = pd.DataFrame.sparse.from_spmatrix(one_hot_sparse, columns=mlb.classes_)\n",
    "\n",
    "# Step 3: Compute frequent itemsets using FP-Growth\n",
    "# Reduce support level if too few frequent itemsets are found\n",
    "support_level = 0.01\n",
    "frequent_itemsets = fpgrowth(one_hot, min_support=support_level, use_colnames=True)\n",
    "\n",
    "# Display frequent itemsets\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Step 4: Generate association rules\n",
    "confidence_level = 0.1\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_level)\n",
    "\n",
    "# Step 5: Display results\n",
    "print(f\"Frequent Itemsets with support >= {support_level:.2%}:\\n{frequent_itemsets}\")\n",
    "print(f\"\\nAssociation Rules with confidence >= {confidence_level:.2%}:\\n{rules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
